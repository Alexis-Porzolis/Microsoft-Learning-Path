{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exercise: Random forests and hyperparameters\n",
        "\n",
        "The goal of this unit is to explore how hyperparameters change training, and thus model performance. The line between model architecture and hyperparameters is a bit blurry for random forests because training itself actually changes the architecture of the model by adding or removing branches. \n",
        "\n",
        "We'll again persue our goal of predicting which crimes in San Francisco will be resolved.\n",
        "\n",
        "## Data and Training Preparation\n",
        "\n",
        "Let's load our data, split it, and prepare for training. This is the same code you've seen in the previous exercises. If you've not done those, go back and do them now!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ready!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DayOfWeek</th>\n",
              "      <th>Resolution</th>\n",
              "      <th>X</th>\n",
              "      <th>Y</th>\n",
              "      <th>day_of_year</th>\n",
              "      <th>time_in_hours</th>\n",
              "      <th>Category_ARSON</th>\n",
              "      <th>Category_ASSAULT</th>\n",
              "      <th>Category_BAD CHECKS</th>\n",
              "      <th>Category_BRIBERY</th>\n",
              "      <th>...</th>\n",
              "      <th>PdDistrict_BAYVIEW</th>\n",
              "      <th>PdDistrict_CENTRAL</th>\n",
              "      <th>PdDistrict_INGLESIDE</th>\n",
              "      <th>PdDistrict_MISSION</th>\n",
              "      <th>PdDistrict_NORTHERN</th>\n",
              "      <th>PdDistrict_PARK</th>\n",
              "      <th>PdDistrict_RICHMOND</th>\n",
              "      <th>PdDistrict_SOUTHERN</th>\n",
              "      <th>PdDistrict_TARAVAL</th>\n",
              "      <th>PdDistrict_TENDERLOIN</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "      <td>True</td>\n",
              "      <td>-122.403405</td>\n",
              "      <td>37.775421</td>\n",
              "      <td>29</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5</td>\n",
              "      <td>True</td>\n",
              "      <td>-122.403405</td>\n",
              "      <td>37.775421</td>\n",
              "      <td>29</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>-122.388856</td>\n",
              "      <td>37.729981</td>\n",
              "      <td>116</td>\n",
              "      <td>14.983333</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>-122.412971</td>\n",
              "      <td>37.785788</td>\n",
              "      <td>5</td>\n",
              "      <td>23.833333</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>False</td>\n",
              "      <td>-122.419672</td>\n",
              "      <td>37.765050</td>\n",
              "      <td>1</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 54 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   DayOfWeek  Resolution           X          Y  day_of_year  time_in_hours  \\\n",
              "0          5        True -122.403405  37.775421           29      11.000000   \n",
              "1          5        True -122.403405  37.775421           29      11.000000   \n",
              "2          1        True -122.388856  37.729981          116      14.983333   \n",
              "3          2       False -122.412971  37.785788            5      23.833333   \n",
              "4          5       False -122.419672  37.765050            1       0.500000   \n",
              "\n",
              "   Category_ARSON  Category_ASSAULT  Category_BAD CHECKS  Category_BRIBERY  \\\n",
              "0           False             False                False             False   \n",
              "1           False             False                False             False   \n",
              "2           False             False                False             False   \n",
              "3           False             False                False             False   \n",
              "4           False             False                False             False   \n",
              "\n",
              "   ...  PdDistrict_BAYVIEW  PdDistrict_CENTRAL  PdDistrict_INGLESIDE  \\\n",
              "0  ...               False               False                 False   \n",
              "1  ...               False               False                 False   \n",
              "2  ...                True               False                 False   \n",
              "3  ...               False               False                 False   \n",
              "4  ...               False               False                 False   \n",
              "\n",
              "   PdDistrict_MISSION  PdDistrict_NORTHERN  PdDistrict_PARK  \\\n",
              "0               False                False            False   \n",
              "1               False                False            False   \n",
              "2               False                False            False   \n",
              "3               False                False            False   \n",
              "4                True                False            False   \n",
              "\n",
              "   PdDistrict_RICHMOND  PdDistrict_SOUTHERN  PdDistrict_TARAVAL  \\\n",
              "0                False                 True               False   \n",
              "1                False                 True               False   \n",
              "2                False                False               False   \n",
              "3                False                False               False   \n",
              "4                False                False               False   \n",
              "\n",
              "   PdDistrict_TENDERLOIN  \n",
              "0                  False  \n",
              "1                  False  \n",
              "2                  False  \n",
              "3                   True  \n",
              "4                  False  \n",
              "\n",
              "[5 rows x 54 columns]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# This code is exactly the same as what we have done in the previous exercises. You do not need to read it again.\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "import graphing # custom graphing code. See our GitHub repo for details\n",
        "\n",
        "#Import the data from the .csv file\n",
        "dataset = pd.read_csv('https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/Data/san_fran_crime.csv', \n",
        "                      delimiter=\"\\t\")\n",
        "\n",
        "# One-hot encode features\n",
        "dataset = pd.get_dummies(dataset, columns=[\"Category\", \"PdDistrict\"], drop_first=False)\n",
        "\n",
        "features = [c for c in dataset.columns if c != \"Resolution\"]\n",
        "\n",
        "# Make a utility method that we can re-use throughout this exercise\n",
        "# To easily fit and test out model\n",
        "def fit_and_test_model(model):\n",
        "    '''\n",
        "    Trains a model and tests it against both train and test sets\n",
        "    '''  \n",
        "    global features\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(train[features], train.Resolution)\n",
        "\n",
        "    # Assess its performance\n",
        "    # -- Train\n",
        "    predictions = model.predict(train[features])\n",
        "    train_accuracy = balanced_accuracy_score(train.Resolution, predictions)\n",
        "\n",
        "    # -- Test\n",
        "    predictions = model.predict(test[features])\n",
        "    test_accuracy = balanced_accuracy_score(test.Resolution, predictions)\n",
        "\n",
        "    return train_accuracy, test_accuracy\n",
        "\n",
        "\n",
        "print(\"Ready!\")\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's not forget to split our data!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the dataset in an 90/10 train/test ratio. \n",
        "train, test = train_test_split(dataset, test_size=0.1, random_state=2, shuffle=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Criteria to split on\n",
        "\n",
        "The first hyperparameter with which we'll work is the criterion. This is essentially a kind of cost function that is used to determine whether a node should be split or not. We have two options available in the package that we are using: `gini` and `entropy`. Let's try them both: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "ename": "UnboundLocalError",
          "evalue": "cannot access local variable 'train' where it is not associated with a value",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[6], line 18\u001b[0m\n\u001b[0;32m     11\u001b[0m rf \u001b[39m=\u001b[39m RandomForestClassifier(n_estimators\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m,\n\u001b[0;32m     12\u001b[0m                             \u001b[39m# max_depth=12,\u001b[39;00m\n\u001b[0;32m     13\u001b[0m                             \u001b[39m# max_features=cur_max_features,\u001b[39;00m\n\u001b[0;32m     14\u001b[0m                             random_state\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[0;32m     15\u001b[0m                             criterion\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgini\u001b[39m\u001b[39m\"\u001b[39m, \n\u001b[0;32m     16\u001b[0m                             verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m     17\u001b[0m \u001b[39m# Train and test the result\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m train_accuracy, test_accuracy \u001b[39m=\u001b[39m fit_and_test_model(rf)\n\u001b[0;32m     19\u001b[0m \u001b[39m# Train and test the result\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39mprint\u001b[39m(train_accuracy, test_accuracy)\n",
            "Cell \u001b[1;32mIn[2], line 26\u001b[0m, in \u001b[0;36mfit_and_test_model\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m     24\u001b[0m test: \u001b[39mstr\u001b[39m\n\u001b[0;32m     25\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m model\u001b[39m.\u001b[39mfit(train[features], train\u001b[39m.\u001b[39mResolution)\n\u001b[0;32m     28\u001b[0m \u001b[39m# Assess its performance\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[39m# -- Train\u001b[39;00m\n\u001b[0;32m     30\u001b[0m predictions \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(train[features])\n",
            "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'train' where it is not associated with a value"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Shrink the training set temporarily to explore this\n",
        "# setting with a more normal sample size\n",
        "sample_size = 1000\n",
        "full_trainset = train\n",
        "train = full_trainset[:sample_size]\n",
        "\n",
        "\n",
        "# Prepare the model \n",
        "rf = RandomForestClassifier(n_estimators=10,\n",
        "                            # max_depth=12,\n",
        "                            # max_features=cur_max_features,\n",
        "                            random_state=2,\n",
        "                            criterion=\"gini\", \n",
        "                            verbose=False)\n",
        "# Train and test the result\n",
        "train_accuracy, test_accuracy = fit_and_test_model(rf)\n",
        "# Train and test the result\n",
        "print(train_accuracy, test_accuracy)\n",
        "\n",
        "# Prepare the model \n",
        "rf = RandomForestClassifier(n_estimators=10,\n",
        "                            random_state=2,\n",
        "                            criterion=\"entropy\", \n",
        "                            verbose=False)\n",
        "# Train and test the result\n",
        "train_accuracy, test_accuracy = fit_and_test_model(rf)\n",
        "# Train and test the result\n",
        "print(train_accuracy, test_accuracy)\n",
        "\n",
        "# Roll back the train dataset to the full train set\n",
        "train = full_trainset\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Results are subtly different, and usually only subtly as both criteria use a similar way to assess performance. We suggest you try different sample sizes, such as 50 and 50000, to see how this changes with larger or smaller samples. \n",
        "\n",
        "## Minimum impurity decrease\n",
        "\n",
        "The minimum impurity decrease is another criterion that is used to assess whether a node should be split. It's used by the `gini` or `entropy` algorithms we used previously. If minimum impurity decrease is high, then splitting a node must result in substantial performance improvement. If it's very low, then nodes can be split even if they offer very little to no performance improvements on the training dataset. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Working...\n"
          ]
        },
        {
          "ename": "UnboundLocalError",
          "evalue": "cannot access local variable 'train' where it is not associated with a value",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[7], line 24\u001b[0m\n\u001b[0;32m     18\u001b[0m rf \u001b[39m=\u001b[39m RandomForestClassifier(n_estimators\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m,\n\u001b[0;32m     19\u001b[0m                             min_impurity_decrease\u001b[39m=\u001b[39mmin_impurity_decrease,\n\u001b[0;32m     20\u001b[0m                             random_state\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, \n\u001b[0;32m     21\u001b[0m                             verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m     23\u001b[0m \u001b[39m# Train and test the result\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m train_accuracy, test_accuracy \u001b[39m=\u001b[39m fit_and_test_model(rf)\n\u001b[0;32m     26\u001b[0m \u001b[39m# Save the results\u001b[39;00m\n\u001b[0;32m     27\u001b[0m test_accuracies\u001b[39m.\u001b[39mappend(test_accuracy)\n",
            "Cell \u001b[1;32mIn[2], line 26\u001b[0m, in \u001b[0;36mfit_and_test_model\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m     24\u001b[0m test: \u001b[39mstr\u001b[39m\n\u001b[0;32m     25\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m model\u001b[39m.\u001b[39mfit(train[features], train\u001b[39m.\u001b[39mResolution)\n\u001b[0;32m     28\u001b[0m \u001b[39m# Assess its performance\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[39m# -- Train\u001b[39;00m\n\u001b[0;32m     30\u001b[0m predictions \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(train[features])\n",
            "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'train' where it is not associated with a value"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Shrink the training set temporarily to explore this\n",
        "# setting with a more normal sample size\n",
        "full_trainset = train\n",
        "train = full_trainset[:1000] # limit to 1000 samples\n",
        "\n",
        "min_impurity_decreases = np.linspace(0, 0.0005, num=100)\n",
        "\n",
        "# Train our models and report their performance\n",
        "train_accuracies = []\n",
        "test_accuracies = []\n",
        "\n",
        "print(\"Working...\")\n",
        "for min_impurity_decrease in min_impurity_decreases:\n",
        "\n",
        "    # Prepare the model \n",
        "    rf = RandomForestClassifier(n_estimators=10,\n",
        "                                min_impurity_decrease=min_impurity_decrease,\n",
        "                                random_state=2, \n",
        "                                verbose=False)\n",
        "    \n",
        "    # Train and test the result\n",
        "    train_accuracy, test_accuracy = fit_and_test_model(rf)\n",
        "\n",
        "    # Save the results\n",
        "    test_accuracies.append(test_accuracy)\n",
        "    train_accuracies.append(train_accuracy)\n",
        "\n",
        "\n",
        "# Plot results\n",
        "graphing.line_2D(dict(Train=train_accuracies, Test=test_accuracies), \n",
        "                    min_impurity_decreases,\n",
        "                    label_x=\"Minimum impurity decreases (min_impurity_decrease)\",\n",
        "                    label_y=\"Accuracy\",\n",
        "                    title=\"Performance\", show=True)\n",
        "\n",
        "# Roll back the train dataset to the full train set\n",
        "train = full_trainset\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice that _train_ performance drastically reduces as we get more scrict about when a node can be split. This is because the higher the minimum impurity decrease, the more strict we are about growing our tree. The smaller the tree, the less overfitting we'll see.\n",
        "\n",
        "Changes in _test_ performance are more subtle. A small increase above zero appears to increase test performance. Further increases begin to hurt test performance only subtly. \n",
        "\n",
        "This is similar to what we saw in the previous exercise about model size; more complex models (those with more nodes) can fit the training data better, but once they exceed a certain complexity, they begin to overfit.\n",
        "\n",
        "\n",
        "## Maximum number of features\n",
        "\n",
        "When trees are created, they are provided with a subset of the data. This not only means they see a certain collection of rows (samples), but also a certain collection of columns (features). The more features are provided, the more likely a given tree is going to overfit. Let's see what happens when we restrict the maximum number of features that can be provided to each tree in the forest:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Working...\n"
          ]
        },
        {
          "ename": "UnboundLocalError",
          "evalue": "cannot access local variable 'train' where it is not associated with a value",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[8], line 22\u001b[0m\n\u001b[0;32m     15\u001b[0m rf \u001b[39m=\u001b[39m RandomForestClassifier(n_estimators\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m,\n\u001b[0;32m     16\u001b[0m                             max_depth\u001b[39m=\u001b[39m\u001b[39m12\u001b[39m,\n\u001b[0;32m     17\u001b[0m                             max_features\u001b[39m=\u001b[39mcur_max_features,\n\u001b[0;32m     18\u001b[0m                             random_state\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, \n\u001b[0;32m     19\u001b[0m                             verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m     21\u001b[0m \u001b[39m# Train and test the result\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m train_accuracy, test_accuracy \u001b[39m=\u001b[39m fit_and_test_model(rf)\n\u001b[0;32m     24\u001b[0m \u001b[39m# Save the results\u001b[39;00m\n\u001b[0;32m     25\u001b[0m test_accuracies\u001b[39m.\u001b[39mappend(test_accuracy)\n",
            "Cell \u001b[1;32mIn[2], line 26\u001b[0m, in \u001b[0;36mfit_and_test_model\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m     24\u001b[0m test: \u001b[39mstr\u001b[39m\n\u001b[0;32m     25\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m model\u001b[39m.\u001b[39mfit(train[features], train\u001b[39m.\u001b[39mResolution)\n\u001b[0;32m     28\u001b[0m \u001b[39m# Assess its performance\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[39m# -- Train\u001b[39;00m\n\u001b[0;32m     30\u001b[0m predictions \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(train[features])\n",
            "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'train' where it is not associated with a value"
          ]
        }
      ],
      "source": [
        "# Shrink the training set temporarily to explore this\n",
        "# setting with a more normal sample size\n",
        "full_trainset = train\n",
        "train = full_trainset[:1000] # limit to 1000 samples\n",
        "\n",
        "max_features = range(10, len(features) +1)\n",
        "\n",
        "# Train our models and report their performance\n",
        "train_accuracies = []\n",
        "test_accuracies = []\n",
        "\n",
        "print(\"Working...\")\n",
        "for cur_max_features in max_features:\n",
        "    # Prepare the model \n",
        "    rf = RandomForestClassifier(n_estimators=50,\n",
        "                                max_depth=12,\n",
        "                                max_features=cur_max_features,\n",
        "                                random_state=2, \n",
        "                                verbose=False)\n",
        "    \n",
        "    # Train and test the result\n",
        "    train_accuracy, test_accuracy = fit_and_test_model(rf)\n",
        "\n",
        "    # Save the results\n",
        "    test_accuracies.append(test_accuracy)\n",
        "    train_accuracies.append(train_accuracy)\n",
        "\n",
        "\n",
        "# Plot results\n",
        "graphing.line_2D(dict(Train=train_accuracies, Test=test_accuracies), \n",
        "                    max_features,\n",
        "                    label_x=\"Maximum number of features (max_features)\",\n",
        "                    label_y=\"Accuracy\",\n",
        "                    title=\"Performance\", show=True)\n",
        "\n",
        "# Roll back the trainset to the full set\n",
        "train = full_trainset"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The more features we have, the worse the overfitting (gap between blue and red lines). Initial increases from 10-20 provide a minute improvement in test performance, after which it begins to hurt test performance very slightly. As features increase, training performance continues to grow unmatched by test performance, indicating overfitting. An optimal value here would be around 20 features."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Seeding\n",
        "\n",
        "Finally, we come to seeding. When trees are initially made, there's a degree of randomness used to decide which features and samples are provided to which trees. Changing the random state (seed) value changes this initial state.\n",
        "\n",
        "The random seed is not a parameter to be tuned, but its effects on our models shouldn't be forgotten, particularly when there isn't much data to work with. Let's see how our model behaves with different random states."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Shrink the training set temporarily to explore this\n",
        "# setting with a more normal sample size\n",
        "sample_size = 1000\n",
        "full_trainset = train\n",
        "train = full_trainset[:sample_size] \n",
        "\n",
        "\n",
        "seeds = range(0,101)\n",
        "\n",
        "# Train our models and report their performance\n",
        "train_accuracies = []\n",
        "test_accuracies = []\n",
        "\n",
        "for seed in seeds:\n",
        "    # Prepare the model \n",
        "    rf = RandomForestClassifier(n_estimators=10,\n",
        "                                random_state=seed, \n",
        "                                verbose=False)\n",
        "    \n",
        "    # Train and test the result\n",
        "    train_accuracy, test_accuracy = fit_and_test_model(rf)\n",
        "\n",
        "    # Save the results\n",
        "    test_accuracies.append(test_accuracy)\n",
        "    train_accuracies.append(train_accuracy)\n",
        "\n",
        "\n",
        "# Plot results\n",
        "graphing.line_2D(dict(Train=train_accuracies, Test=test_accuracies), \n",
        "                    seeds,\n",
        "                    label_x=\"Seed value (random_state)\",\n",
        "                    label_y=\"Accuracy\",\n",
        "                    title=\"Performance\", show=True)\n",
        "\n",
        "# Roll back the trainset to the full set\n",
        "train = full_trainset"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Performance, particularly on the test set, is variable, and thus some part of performance is blind luck. This is not only because the initial state of the model can be different, but also that we split our training and test data differently. Whether this would apply to a holdout set is not easy to tell without trying it.\n",
        "\n",
        "There's no correlation between high or low seed values and performance; seed is not something to \"tune.\" The seed is a random factor and it can help or hinder depending on the model at play. Generally speaking, when we work with small amounts of data, we're more likely to be affected by different seed values. More complex models can also be affected more by the seed, but not always.\n",
        "\n",
        "Try changing the sample size and/or number of trees in the preceding model and watch how the variability in performance changes. Think about why this might be.\n",
        "\n",
        "## Summary\n",
        "\n",
        "Complex models typically have associated hyperparameters that can affect training. The extent to which these matter, and how they affect the result, depends on the data at hand and complexity of the model being used. We usually need to experiment somewhat with these in order to achieve optimum performance for the data that we have."
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "conda-env-azureml_py38-py"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
